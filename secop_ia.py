# -*- coding: utf-8 -*-
"""SECOP_AI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kN_r8aoB6p5KCYj7KsqJTqz_QmR5IKh
"""

# SECOP II

# 1. Installation
# Only run if you don't have the libraries installed
!pip install pandas requests

import pandas as pd
import requests
from datetime import datetime
import time

def search_contracts(limit=100, **filters):
    """
    Search contracts in SECOP II with customizable filters
    """
    url = "https://www.datos.gov.co/resource/rpmr-utcd.json"
    # Unlimited SODA 2.1 alternative: https://www.datos.gov.co/api/v3/views/rpmr-utcd/query.json

    # Build query parameters
    params = {"$limit": limit}

    # Parameter name mapping (adjusted to real SECOP columns)
    field_mapping = {
        'entity_level': 'nivel_entidad',
        'entity_code': 'codigo_entidad_en_secop',
        'entity_name': 'nombre_de_la_entidad',
        'entity_nit': 'nit_de_la_entidad',
        'department': 'departamento_entidad',
        'municipality': 'municipio_entidad',
        'process_status': 'estado_del_proceso',
        'modality': 'modalidad_de_contrataci_n',
        'contract_object': 'objeto_a_contratar',
        'process_object': 'objeto_del_proceso',
        'process_number': 'numero_de_proceso',
        'contract_value': 'valor_contrato',
        'contractor_name': 'nom_raz_social_contratista',
        'contract_url': 'url_contrato',
        'origin': 'origen',
        'document_type': 'tipo_documento_proveedor',
        'provider_document': 'documento_proveedor',
        'signature_date': 'fecha_de_firma_del_contrato',
        'start_date': 'fecha_inicio_ejecuci_n',
        'end_date': 'fecha_fin_ejecuci_n',
    }

    # Apply simple filters
    for filter_key, filter_value in filters.items():
        if filter_value and filter_key in field_mapping:
            api_field = field_mapping[filter_key]
            if 'object' in filter_key:
                params[api_field] = {'$like': f'%{filter_value}%'}
            else:
                params[api_field] = filter_value

    # Build WHERE filters for ranges
    where_conditions = []

    if filters.get('minimum_value', 0) > 0:
        where_conditions.append(f"valor_contrato >= {filters['minimum_value']}")

    if filters.get('maximum_value', 0) > 0:
        where_conditions.append(f"valor_contrato <= {filters['maximum_value']}")

    if filters.get('signature_date_from'):
        where_conditions.append(f"fecha_de_firma_del_contrato >= '{filters['signature_date_from']}'")

    if filters.get('signature_date_to'):
        where_conditions.append(f"fecha_de_firma_del_contrato <= '{filters['signature_date_to']}'")

    if filters.get('start_date_from'):
        where_conditions.append(f"fecha_inicio_ejecuci_n >= '{filters['start_date_from']}'")

    if filters.get('start_date_to'):
        where_conditions.append(f"fecha_inicio_ejecuci_n <= '{filters['start_date_to']}'")

    if where_conditions:
        params['$where'] = ' AND '.join(where_conditions)

    # Make request with retries
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            response = requests.get(url, params=params, timeout=30)

            if response.status_code == 200:
                data = response.json()
                return pd.DataFrame(data) if data else pd.DataFrame()

            elif response.status_code == 429:  # Rate limit
                if attempt < max_attempts - 1:
                    time.sleep(2 ** attempt)
                    continue

            print(f"HTTP Error: {response.status_code}")
            return pd.DataFrame()

        except Exception as e:
            if attempt < max_attempts - 1:
                time.sleep(2)
                continue
            print(f"Error: {e}")
            return pd.DataFrame()

    return pd.DataFrame()

def show_available_columns():
    """Shows available columns in SECOP II"""
    sample_df = search_contracts(limit=1)
    if not sample_df.empty:
        print("Available columns in SECOP II:")
        for i, col in enumerate(sample_df.columns, 1):
            print(f"{i:2d}. {col}")
    return sample_df.columns.tolist() if not sample_df.empty else []

def export_excel(df, prefix="secop"):
    """Export DataFrame to Excel with timestamp"""
    if df.empty:
        print("No data to export")
        return None

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{prefix}_{timestamp}.xlsx"

    try:
        df.to_excel(filename, index=False)
        return filename
    except Exception as e:
        print(f"Export error: {e}")
        return None

def search_summary(df):
    """Show results summary"""
    if df.empty:
        print("No contracts found")
        return

    print(f"Found: {len(df)} contracts")

    # Show first 10 columns
    cols_to_show = list(df.columns)[:10]
    if len(df.columns) > 10:
        cols_to_show.append(f"... and {len(df.columns) - 10} more")
    print(f"Columns: {cols_to_show}")

    # Check contract object
    if 'objeto_a_contratar' in df.columns:
        valid_objects = df['objeto_a_contratar'].notna().sum()
        defined_objects = df[df['objeto_a_contratar'] != 'NO DEFINIDO']['objeto_a_contratar'].notna().sum()
        print(f"Contracts with defined object: {defined_objects}/{len(df)} ({valid_objects} valid)")

# ==============================================================================
# EXECUTION BLOCK - MODIFY FILTERS HERE
# ==============================================================================

# Filter variables
limit = 100
entity_level = ""
entity_code = ""
entity_name = ""
entity_nit = ""
department = "La Guajira"
municipality = ""
process_status = ""
modality = ""
contract_object = ""
process_object = ""
process_number = ""
contract_value = ""
contractor_name = ""
contract_url = ""
origin = ""
document_type = ""
provider_document = ""
minimum_value = 0
maximum_value = 0
signature_date_from = ""
signature_date_to = ""
start_date_from = ""
start_date_to = ""

# Execute search
df = search_contracts(
    limit=limit,
    entity_level=entity_level,
    entity_code=entity_code,
    entity_name=entity_name,
    entity_nit=entity_nit,
    department=department,
    municipality=municipality,
    process_status=process_status,
    modality=modality,
    contract_object=contract_object,
    process_object=process_object,
    process_number=process_number,
    contract_value=contract_value,
    contractor_name=contractor_name,
    contract_url=contract_url,
    origin=origin,
    document_type=document_type,
    provider_document=provider_document,
    minimum_value=minimum_value,
    maximum_value=maximum_value,
    signature_date_from=signature_date_from,
    signature_date_to=signature_date_to,
    start_date_from=start_date_from,
    start_date_to=start_date_to
)

# Save results
if not df.empty:
    filename = export_excel(df)
    if filename:
        print(f"Saved to: {filename}")
    search_summary(df)
else:
    print("No contracts found")

# Quick analysis
df

# SECOP contract pattern analysis
# Run after obtaining the DataFrame df

# Prepare data
df['valor_contrato'] = pd.to_numeric(df['valor_contrato'], errors='coerce')

print("\n--- PATTERN ANALYSIS ---\n")

# 1. MOST FREQUENT CONTRACTORS (by unique document) - IMPROVED VERSION
print("üîç MOST FREQUENT CONTRACTORS:")
print("-" * 60)

# Filter records with valid document (real numbers, not generic texts)
valid_docs = df[
    df['documento_proveedor'].notna() &
    (df['documento_proveedor'] != '') &
    (df['documento_proveedor'] != 'NO DEFINIDO') &
    (df['documento_proveedor'].str.len() >= 7) &  # Minimum 7 digits
    df['documento_proveedor'].str.isdigit()  # Only numbers
].copy()

if not valid_docs.empty:
    # Group by provider document
    analysis_by_doc = valid_docs.groupby('documento_proveedor').agg({
        'nom_raz_social_contratista': ['count', 'first'],
        'valor_contrato': 'sum'
    }).round(0)

    # Simplify column names
    analysis_by_doc.columns = ['num_contracts', 'main_name', 'total_value']

    # Show top 10 contractors by frequency
    top_contractors = analysis_by_doc.sort_values('num_contracts', ascending=False).head(10)

    for i, (document, row) in enumerate(top_contractors.iterrows(), 1):
        print(f"{i:2d}. DOC: {document}")
        print(f"    Name: {row['main_name'][:50]}")
        print(f"    Contracts: {int(row['num_contracts'])}")
        print(f"    Total value: ${row['total_value']:,.0f}")

        # Check for name variations
        variations = valid_docs[valid_docs['documento_proveedor'] == document]['nom_raz_social_contratista'].unique()
        if len(variations) > 1:
            print(f"    ‚ö†Ô∏è  Name variations ({len(variations)}):")
            for variation in variations[:3]:
                print(f"       - {variation[:45]}")
            if len(variations) > 3:
                print(f"       ... and {len(variations) - 3} more")

        # NEW: Show contract URLs directly here
        contractor_contracts = valid_docs[valid_docs['documento_proveedor'] == document]
        contract_urls = []

        # Check all possible URL columns
        possible_url_columns = ['url_contrato', 'url_del_proceso', 'urlproceso', 'link_contrato']

        for col in possible_url_columns:
            if col in contractor_contracts.columns:
                temp_urls = contractor_contracts[col].dropna()
                valid_urls = [str(url).strip() for url in temp_urls if url and str(url).strip() != '' and str(url).lower() != 'nan']
                contract_urls.extend(valid_urls)

        # Remove duplicates while maintaining order
        contract_urls = list(dict.fromkeys(contract_urls))

        # Filter URLs that seem valid (contain http or www)
        active_urls = [url for url in contract_urls if 'http' in url.lower() or 'www' in url.lower()]

        if active_urls:
            print(f"    üìã Contract URLs ({len(active_urls)}):")
            for j, url in enumerate(active_urls[:5], 1):
                print(f"       {j}. {url}")
            if len(active_urls) > 5:
                print(f"       ... and {len(active_urls) - 5} more")
        else:
            print(f"    üìã No active URLs available")

        print()

else:
    print("‚ùå No valid documents to analyze")

# 2. DATA QUALITY
print(f"\nüîç DATA QUALITY:")
print("-" * 40)
total_records = len(df)
no_document = len(df[df['documento_proveedor'].isna() | (df['documento_proveedor'] == '')])
not_defined = len(df[df['documento_proveedor'] == 'NO DEFINIDO'])
non_numeric = len(df[~df['documento_proveedor'].str.isdigit() | (df['documento_proveedor'].str.len() < 7)])
valid_documents = len(valid_docs)
unique_documents = df['documento_proveedor'].nunique()
unique_names = df['nom_raz_social_contratista'].nunique()

print(f"Total records: {total_records}")
print(f"No document: {no_document} ({no_document/total_records*100:.1f}%)")
print(f"'NOT DEFINED': {not_defined} ({not_defined/total_records*100:.1f}%)")
print(f"Non-numeric or short documents: {non_numeric} ({non_numeric/total_records*100:.1f}%)")
print(f"Valid documents for analysis: {valid_documents} ({valid_documents/total_records*100:.1f}%)")
print(f"Total unique documents: {unique_documents}")
print(f"Unique names: {unique_names}")
if valid_documents > 0:
    unique_valid_docs = valid_docs['documento_proveedor'].nunique()
    print(f"Unique valid documents: {unique_valid_docs}")
    print(f"Valid docs/names ratio: {unique_valid_docs/unique_names:.2f}")

# 3. VALUE STATISTICS
print(f"\nüí∞ VALUE STATISTICS:")
print("-" * 40)
valid_values = df[df['valor_contrato'].notna()]
if not valid_values.empty:
    average = valid_values['valor_contrato'].mean()
    median = valid_values['valor_contrato'].median()
    above_average = valid_values[valid_values['valor_contrato'] > average]

    print(f"Average value: ${average:,.0f}")
    print(f"Median value: ${median:,.0f}")
    print(f"Contracts above average: {len(above_average)}/{len(valid_values)} ({len(above_average)/len(valid_values)*100:.1f}%)")

# 4. HIGHEST VALUE CONTRACTS
print(f"\nüèÜ HIGHEST VALUE CONTRACTS:")
print("-" * 50)
valid_values = df[df['valor_contrato'].notna()]
if not valid_values.empty:
    top_values = valid_values.nlargest(5, 'valor_contrato')
    for i, (_, row) in enumerate(top_values.iterrows(), 1):
        print(f"{i}. ${row['valor_contrato']:,.0f}")
        print(f"   Contractor: {row['nom_raz_social_contratista'][:40]}")
        print(f"   Entity: {row['nombre_de_la_entidad'][:35]}")
        if 'documento_proveedor' in row and pd.notna(row['documento_proveedor']):
            print(f"   Document: {row['documento_proveedor']}")
        print()

# 5. CONTRACTING MODALITIES
print("\nüìã MOST COMMON MODALITIES:")
print("-" * 40)
if 'modalidad_de_contrataci_n' in df.columns:
    modalities = df['modalidad_de_contrataci_n'].value_counts().head(5)
    for modality, freq in modalities.items():
        pct = (freq / len(df)) * 100
        print(f"{modality}: {freq} ({pct:.1f}%)")

# 6. PREPARATION FOR WEB RESEARCH
def prepare_web_research(num_top=3):
    """Prepare data for web research"""
    if valid_docs.empty:
        return []

    top_to_research = analysis_by_doc.sort_values('num_contracts', ascending=False).head(num_top)
    contractors_to_research = []

    for i, (document, row) in enumerate(top_to_research.iterrows(), 1):
        contractor_contracts = valid_docs[valid_docs['documento_proveedor'] == document]
        all_variations = contractor_contracts['nom_raz_social_contratista'].unique()
        most_complete_name = max(all_variations, key=len)

        # Get URLs
        contract_urls = []
        for col in ['url_contrato', 'url_del_proceso', 'urlproceso', 'link_contrato']:
            if col in contractor_contracts.columns:
                temp_urls = contractor_contracts[col].dropna()
                valid_urls = [str(url).strip() for url in temp_urls if url and str(url).strip() != '' and str(url).lower() != 'nan']
                contract_urls.extend(valid_urls)

        active_urls = [url for url in list(dict.fromkeys(contract_urls)) if 'http' in url.lower() or 'www' in url.lower()]

        contractor_info = {
            'ranking': i,
            'document': document,
            'main_name': row['main_name'],
            'complete_name': most_complete_name,
            'num_contracts': int(row['num_contracts']),
            'total_value': row['total_value'],
            'name_variations': list(all_variations),
            'contract_urls': active_urls
        }
        contractors_to_research.append(contractor_info)

    return contractors_to_research

# Execute preparation (no visible output)
contractors_for_web = prepare_web_research(3)

# SECOP contractor web research
# Run after pattern analysis

# Installations
!pip install langchain langchain-openai tavily-python requests beautifulsoup4

from tavily import TavilyClient
from langchain_openai import ChatOpenAI
import requests
from bs4 import BeautifulSoup
from google.colab import userdata
import time

# ============================================================================
# CONFIGURATION
# ============================================================================

NUM_TOP_CONTRACTORS = 3  # Number of contractors to research
MAX_WEB_SOURCES = 5      # Number of web sources per contractor

# Database of connections for graph
found_connections = []

# API Configuration
TAVILY_KEY = userdata.get('TAVILY_KEY')
OPENAI_KEY = userdata.get('OPENAI_API_KEY')

llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=OPENAI_KEY)
tavily = TavilyClient(api_key=TAVILY_KEY)

def web_search(query, max_results=None):
    """Search web information about the contractor"""
    if max_results is None:
        max_results = MAX_WEB_SOURCES

    try:
        response = tavily.search(
            query=query,
            max_results=max_results,
            topic="general",
            location="CO",
            language="es"
        )

        # Build content with URLs
        content_with_sources = []
        source_urls = []

        for r in response.get("results", []):
            title = r.get('title', '')
            content = r.get('content', '')[:400]
            url = r.get('url', '')

            content_with_sources.append(f"**{title}**\n{content}...\nSource: {url}")
            source_urls.append(url)

        return "\n\n".join(content_with_sources), source_urls

    except Exception as e:
        return f"Search error: {e}", []

def scrape_secop_contract(url):
    """Extract basic information from a SECOP contract"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            return "Could not access contract"

        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract relevant information
        info = []

        # Search for common SECOP fields
        fields = ['objeto', 'modalidad', 'estado', 'valor', 'plazo', 'supervisor']
        for field in fields:
            elements = soup.find_all(text=lambda x: x and field.lower() in x.lower())
            if elements:
                info.append(f"{field.title()}: {elements[0][:100]}")

        return "\n".join(info) if info else "Information not available"

    except Exception as e:
        return f"Access error: {str(e)}"

def research_contractor_by_document(document, search_name, contractor_info):
    """Generate web report of contractor based on unique document"""

    # Contractor data in SECOP
    contractor_contracts = df[df['documento_proveedor'] == document]
    num_contracts = len(contractor_contracts)
    total_value = contractor_contracts['valor_contrato'].sum()

    # Get additional information
    name_variations = contractor_contracts['nom_raz_social_contratista'].unique()
    contracting_entities = contractor_contracts['nombre_de_la_entidad'].unique()

    print(f"\n{'='*60}")
    print(f"üîç RESEARCHING: {search_name}")
    print(f"üìÑ DOCUMENT: {document}")
    print(f"{'='*60}")
    print(f"üìä Contracts in SECOP: {num_contracts}")
    print(f"üí∞ Total value: ${total_value:,.0f}")
    print(f"üìù Name variations: {len(name_variations)}")

    # Show variations if any
    if len(name_variations) > 1:
        print(f"   Registered names:")
        for var in name_variations:
            freq = len(contractor_contracts[contractor_contracts['nom_raz_social_contratista'] == var])
            print(f"   - {var} ({freq} contracts)")

    # SECOP contract information
    secop_contract_url = ""
    contract_info = ""
    if 'url_contrato' in contractor_contracts.columns:
        valid_urls = contractor_contracts['url_contrato'].dropna()
        if not valid_urls.empty:
            secop_contract_url = valid_urls.iloc[0]
            print(f"üîó Analyzing contract: {secop_contract_url}")
            contract_info = scrape_secop_contract(secop_contract_url)

    # Search web information
    print(f"üåê Searching web information...")
    web_information, source_urls = web_search(f'"{search_name}" Colombia NIT {document}')

    # Alternative search if no results
    if "Search error" in web_information or len(web_information) < 100:
        print(f"üîÑ Alternative search...")
        web_information, source_urls = web_search(f'"{search_name}" company Colombia')

    # Generate structured report - IMPROVED VERSION
    prompt = f"""
    Analyze the following information about the contractor with document {document}:

    SECOP DATA:
    - Provider document: {document}
    - Main name: {search_name}
    - Name variations: {', '.join(name_variations)}
    - Number of contracts: {num_contracts}
    - Total contracted value: ${total_value:,.0f}
    - Main contracting entities: {', '.join(contracting_entities[:5])}
    - SECOP contract URL: {secop_contract_url}

    CONTRACT INFORMATION:
    {contract_info}

    WEB INFORMATION:
    {web_information}

    Generate a structured report with narrative and detailed style:

    1. **EXECUTIVE SUMMARY**
    Write 4-5 lines that capture the essence of the contractor: what type of company it is, its level of activity in public contracting, its specialization and relevant aspects that characterize it in the state contracting ecosystem.

    2. **COMPANY IDENTIFICATION**
    Develop a complete description including:
       - Main corporate name and identified commercial names
       - NIT/Document: {document}
       - If name variations exist, explain in detail the differences found and possible reasons (corporate name changes, transcription errors, abbreviations, etc.)
       - Geographic location and approximate age if available

    3. **MAIN ECONOMIC ACTIVITY**
    Provide an in-depth analysis of the economic sector in which it operates, types of products or services it offers based on contractual objects, its specialization niche within the public contracting market, and technical capabilities evidenced through its contracts.

    4. **CONTRACTUAL ACTIVITY ANALYSIS**
    Develop a complete overview of its performance in public contracting:
       - Narrative description of contracting frequency ({num_contracts} contracts)
       - Analysis of value ranges it usually handles
       - Identification and description of entities it contracts with most
       - Temporal patterns and types of modalities it uses most
       - Diversification of its institutional client portfolio

    5. **RELATED PEOPLE AND ENTITIES**
    Research exhaustively and present ALL people and entities linked to the contractor found in the web information. For each person or entity identified provide a mini-description:
       - Full names of mentioned people (representatives, partners, directors, etc.) with description of their role and responsibilities
       - Specific positions or roles and any information about their trajectory
       - Associated companies or entities with type of relationship (subsidiary, parent company, ally, supplier) and description of the connection
       - Any business or family connection mentioned with context
       - Trade organizations, chambers of commerce or associations

    For each person/entity found, use the format:
    - Full name | Position/role | Mini-description of relationship with company

    IMPORTANT: After each main section, immediately include:
    Source: SPECIFIC_URL

    Use professional but fluid language, integrating information coherently. Where SPECIFIC_URL is the complete URL that supports that information.
    DO NOT use asterisks or parentheses around URLs.
    """

    response = llm.invoke(prompt)

    # Extract connections for the graph
    connections_prompt = f"""
    From the following report about the contractor with document {document}, extract only:

    {response.content}

    List ONLY the names of people and entities mentioned in format:
    PERSON: full name | position/role | base document: {document}
    ENTITY: entity name | relationship | base document: {document}

    Respond only with the list, without additional explanations.
    """

    connections_response = llm.invoke(connections_prompt)

    # Save connections
    connection_data = {
        'main_document': document,
        'main_contractor': search_name,
        'name_variations': list(name_variations),
        'connections_text': connections_response.content,
        'total_value': total_value,
        'num_contracts': num_contracts,
        'contracting_entities': list(contracting_entities)
    }
    found_connections.append(connection_data)

    # Build complete report with sources
    complete_report = response.content

    complete_report += "\n\n" + "="*60
    complete_report += "\nüîó CONSULTED SOURCES:"
    complete_report += "\n" + "="*60

    if secop_contract_url:
        complete_report += f"\nüìã SECOP: {secop_contract_url}"

    for i, url in enumerate(source_urls, 1):
        complete_report += f"\nüåê Source {i}: {url}"

    complete_report += f"\n\nüìÑ PROVIDER DOCUMENT: {document}"
    complete_report += f"\nüìä TOTAL CONTRACTS ANALYZED: {num_contracts}"

    return complete_report

def execute_automatic_research():
    """Automatically research top contractors"""

    if not contractors_for_web:
        print("‚ùå No contractors prepared for research")
        return

    print("="*70)
    print(f"üîç AUTOMATIC WEB RESEARCH")
    print("="*70)

    for contractor_info in contractors_for_web:
        document = contractor_info['document']
        search_name = contractor_info['complete_name']

        report = research_contractor_by_document(document, search_name, contractor_info)
        print(report)

        if contractor_info != contractors_for_web[-1]:
            print("\n" + "üîÑ NEXT CONTRACTOR..." + "\n")
            time.sleep(2)  # Pause between searches

    print("\n" + "="*70)
    print("‚úÖ AUTOMATIC RESEARCH COMPLETED")
    print("="*70)

def execute_manual_research(provider_document):
    """Research a specific contractor by their document"""

    # Search contractor by document
    found_contracts = df[df['documento_proveedor'] == provider_document]

    if found_contracts.empty:
        print(f"‚ùå No contracts found for document: {provider_document}")
        return

    # Get the most complete name
    names = found_contracts['nom_raz_social_contratista'].unique()
    search_name = max(names, key=len)

    print("\n" + "="*70)
    print("üîç MANUAL RESEARCH")
    print("="*70)

    # Create contractor info
    contractor_info = {
        'document': provider_document,
        'complete_name': search_name,
        'num_contracts': len(found_contracts),
        'total_value': found_contracts['valor_contrato'].sum()
    }

    report = research_contractor_by_document(provider_document, search_name, contractor_info)
    print(report)

    print("\n" + "="*70)
    print("‚úÖ MANUAL RESEARCH COMPLETED")
    print("="*70)

# ============================================================================
# EXECUTION
# ============================================================================

# Execute automatic research
execute_automatic_research()

# ============================================================================
# SHOW FOUND CONNECTIONS
# ============================================================================

print("\n" + "="*70)
print("üîó CONNECTIONS FOUND FOR GRAPH")
print("="*70)

for i, connection in enumerate(found_connections, 1):
    print(f"\n{i}. {connection['main_contractor']} (DOC: {connection['main_document']}):")
    print(f"   Contracts: {connection['num_contracts']} | Value: ${connection['total_value']:,.0f}")
    print(f"   Connections:\n{connection['connections_text']}")
    print("-" * 50)

# ============================================================================
# MANUAL RESEARCH (OPTIONAL)
# ============================================================================

# To research a specific contractor, use:
# execute_manual_research("DOCUMENT_NUMBER_HERE")
